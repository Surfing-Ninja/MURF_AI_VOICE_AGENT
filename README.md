# ğŸ™ï¸ Multimodal Voice Agent - Full Stack

A high-performance, real-time voice-to-voice customer support agent with **Sight** (Vision), **Memory** (RAG), and **Action** (Agentic Tools).

Built with **Node.js** backend + **React + Vite** frontend.

## ğŸ¯ Features

- ğŸ¤ **Real-time Speech-to-Text**: Deepgram Nova-2 with 250ms streaming
- ğŸ§  **AI Brain**: Gemini 2.0 Flash-Lite with emotion-aware responses
- ğŸ”Š **Text-to-Speech**: Murf.ai Falcon (ready for integration)
- ğŸ“š **RAG Memory**: Pinecone vector database with knowledge retrieval
- ğŸ‘ï¸ **Multimodal Vision**: Screen capture analysis capability
- ğŸ”§ **Agentic Tools**: Execute actions (refunds, updates, etc.)
- âš¡ **Low Latency**: 250ms audio chunks + speech_final detection

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend â”‚  â€¢ useAudioRecorder (250ms timeslice)
â”‚   (Port 5173)   â”‚  â€¢ WebSocket connection
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Audio streaming
         â”‚ 
         â”‚ WebSocket (ws://localhost:3000)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Node.js Server â”‚  â€¢ Direct Deepgram integration
â”‚   (Port 3000)   â”‚  â€¢ AI Controller (speech_final check)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â€¢ Session management
         â†“
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“        â†“        â†“
Deepgram  Gemini    Murf   Pinecone
 Nova-2   Flash    Falcon    RAG
```

## ğŸ“ Project Structure

```
Voice_agent_bot/
â”‚
â”œâ”€â”€ server/                          # Backend (Node.js + Express)
â”‚   â”œâ”€â”€ server.js                    # Main orchestrator
â”‚   â”œâ”€â”€ aiController.js              # Strict TOON format AI
â”‚   â”œâ”€â”€ services/                    # Core services
â”‚   â”‚   â”œâ”€â”€ deepgramService.js       # STT
â”‚   â”‚   â”œâ”€â”€ geminiService.js         # LLM
â”‚   â”‚   â”œâ”€â”€ murfService.js           # TTS
â”‚   â”‚   â””â”€â”€ ragService.js            # Vector DB
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ ingest.js                # RAG ingestion
â”‚   â””â”€â”€ README.md                    # Backend docs
â”‚
â”œâ”€â”€ client/                          # Frontend (React + Vite) â­
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                  # Main component
â”‚   â”‚   â””â”€â”€ hooks/
â”‚   â”‚       â”œâ”€â”€ useAudioRecorder.js  # â­ Audio hook (250ms)
â”‚   â”‚       â””â”€â”€ useWebSocket.js      # WebSocket hook
â”‚   â””â”€â”€ README.md                    # Frontend docs
â”‚
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ installed
- API keys for:
  - Deepgram (Speech-to-Text)
  - Google Gemini (AI)
  - Murf.ai (Text-to-Speech)
  - Pinecone (Vector DB)

### 1. Clone Repository

```bash
git clone <your-repo>
cd Voice_agent_bot
```

### 2. Backend Setup

```bash
cd server
npm install

# Configure API keys in .env
# Already done if you followed initial setup

# Ingest sample knowledge
node scripts/ingest.js ./documents/sample_knowledge.txt

# Start backend
npm start
```

âœ… Backend running on: **http://localhost:3000**

### 3. Frontend Setup

```bash
cd ../client
npm install

# Start frontend
npm run dev
```

âœ… Frontend running on: **http://localhost:5173**

### 4. Test the System

1. Open browser: **http://localhost:5173**
2. Click **"Start Talking"**
3. Say: *"What is your refund policy?"*
4. Wait for AI response!

## ğŸ¤ Frontend Features

### useAudioRecorder Hook

The core audio recording hook with:

- âœ… **250ms timeslice** for low-latency streaming
- âœ… **Automatic WebSocket streaming** of audio chunks
- âœ… **Microphone permission handling**
- âœ… **Error handling** with user-friendly messages
- âœ… **Proper cleanup** on stop

**API:**
```javascript
const { startRecording, stopRecording, isRecording, error } = useAudioRecorder(websocket);
```

**Audio Configuration:**
- Sample Rate: 16kHz (Deepgram requirement)
- Channels: Mono (1 channel)
- Format: audio/webm;codecs=opus
- Bitrate: 128 kbps

**Key Implementation:**
```javascript
// MediaRecorder with 250ms timeslice
mediaRecorder.ondataavailable = (event) => {
  if (event.data.size > 0 && websocket.readyState === WebSocket.OPEN) {
    websocket.send(event.data);  // Send binary Blob
  }
};

mediaRecorder.start(250);  // 250ms chunks
```

## ğŸ”§ Backend Features

### Direct Deepgram Integration

- Uses `@deepgram/sdk` with `listen.live()`
- Configuration: `nova-2`, `smart_format`, `linear16` @ 16kHz
- **Critical**: Only processes when `speech_final=true`

### AI Controller (Strict TOON)

- Model: Gemini 2.0 Flash-Lite
- Format: `response[1]{text,emotion}: text,emotion`
- Max 20 words per response
- 4 emotions: happy, sad, angry, neutral

### speech_final Behavior

```javascript
if (transcript && transcript.trim().length > 0 && speechFinal) {
  // User finished speaking - generate response
  const aiResponse = await generateResponse(transcript);
} else if (transcript && transcript.trim().length > 0) {
  // Interim result - log but don't process
  console.log('ğŸ¤ Interim:', transcript);
}
```

## ğŸ“Š Data Flow

```
1. User speaks into microphone
   â†“
2. useAudioRecorder captures audio (250ms chunks)
   â†“
3. WebSocket sends binary audio to server
   â†“
4. Deepgram transcribes (waits for speech_final=true)
   â†“
5. AI Controller generates TOON response
   â†“
6. Server sends transcript + AI response to client
   â†“
7. Frontend displays both
   â†“
8. TODO: Murf TTS converts to audio
   â†“
9. TODO: Client plays audio response
```

## ğŸŒ WebSocket Protocol

### Client â†’ Server

**Binary Audio Data:**
```
Blob (audio/webm)
Sent every 250ms
```

**Control Messages:**
```json
{ "type": "start_recording", "timestamp": 1234567890 }
{ "type": "stop_recording", "timestamp": 1234567890 }
```

### Server â†’ Client

```json
// Transcript
{
  "type": "transcript",
  "text": "What is your refund policy?",
  "isFinal": true,
  "timestamp": 1234567890
}

// AI Response
{
  "type": "ai_response",
  "text": "We offer a 30-day money-back guarantee",
  "emotion": "neutral",
  "timestamp": 1234567890
}

// Error
{
  "type": "error",
  "message": "Error message",
  "timestamp": 1234567890
}
```

## ğŸ¨ UI Components

### Recording Button
- Idle: Blue gradient with ğŸ¤ icon
- Recording: Red gradient with ğŸ”´ icon (pulsing)
- Disabled when WebSocket disconnected

### Status Indicators
- Connection status (green = connected, red = disconnected)
- Recording indicator with pulse animation

### Display Sections
- **Transcript**: User's speech with ğŸ‘¤ icon
- **AI Response**: Agent's reply with ğŸ¤– icon + emotion badge
- **Instructions**: Help text when idle

## ğŸ› Debugging

### Backend Logs

```
ğŸ¤ Deepgram Transcript (speech_final): "..."
ğŸ§  Calling AI Controller...
âœ“ AI Response - Text: "..." / Emotion: neutral
```

### Frontend Logs

```
ğŸ“¤ Sending audio chunk: 32768 bytes
ğŸ“¨ Received: transcript
ğŸ“¨ Received: ai_response
```

## âš ï¸ Known Limitations

- [ ] TTS (Murf) not yet integrated in frontend
- [ ] No audio playback of AI responses yet
- [ ] No conversation history UI
- [ ] No screen capture in React frontend
- [ ] No audio visualization

## ğŸš€ Next Steps

### High Priority
1. Integrate Murf TTS in frontend
2. Add audio playback for AI responses
3. Implement conversation history UI

### Medium Priority
4. Add audio visualization (waveform/spectrum)
5. Implement screen capture for vision
6. Add push-to-talk mode option
7. Support voice activity detection (VAD)

### Low Priority
8. Add user authentication
9. Implement conversation export
10. Add analytics dashboard

## ğŸ“š Documentation

- **Backend**: `/server/README.md` - Complete backend documentation
- **Frontend**: `/client/README.md` - Complete frontend documentation
- **This File**: Overview and quick start

## ğŸ§ª Testing

### Manual Test Flow

1. **WebSocket Connection**:
   - Start backend
   - Start frontend
   - Verify green status indicator

2. **Audio Recording**:
   - Click "Start Talking"
   - Allow microphone permissions
   - Speak for 3+ seconds
   - Check console for audio chunks

3. **Transcription**:
   - Speak clearly: "What is your refund policy?"
   - Wait for transcript to appear
   - Verify speech_final triggered

4. **AI Response**:
   - Verify AI response appears below transcript
   - Check emotion badge displays correctly
   - Verify response is under 20 words

5. **Error Handling**:
   - Deny mic permissions â†’ See error banner
   - Kill backend â†’ Verify auto-reconnect
   - Disconnect mic â†’ See error message

## ğŸ“Š Performance

- **Audio Latency**: ~250ms (chunk size)
- **Transcription**: ~500ms (Deepgram processing)
- **AI Response**: ~1-2s (Gemini generation)
- **Total Latency**: ~2-3s from speech end to response

## ğŸ” Security

- HTTPS required for `getUserMedia()` in production
- Use WSS (secure WebSocket) in production
- API keys stored in `.env` (never commit)
- No audio data stored (privacy-first)

## ğŸŒ Deployment

### Backend (Node.js)

```bash
# Build
cd server
npm install

# Deploy to Heroku/Railway/Fly.io
# Set environment variables
# Update WebSocket URL
```

### Frontend (React)

```bash
# Build
cd client
npm run build

# Deploy to Vercel/Netlify/Cloudflare Pages
# Update VITE_WS_URL to production backend
```

## ğŸ“ Environment Variables

### Backend (.env)

```bash
DEEPGRAM_API_KEY=your_key
GEMINI_API_KEY=your_key
MURF_API_KEY=your_key
PINECONE_API_KEY=your_key
PINECONE_ENVIRONMENT=your_env
PINECONE_INDEX=voice-agent-kb
```

### Frontend (.env)

```bash
VITE_WS_URL=ws://localhost:3000  # Development
# VITE_WS_URL=wss://your-backend.com  # Production
```

## ğŸ› ï¸ Tech Stack

### Backend
- Node.js + Express
- WebSocket (ws library)
- Deepgram SDK (@deepgram/sdk)
- Google Generative AI (@google/generative-ai)
- Pinecone SDK (@pinecone-database/pinecone)

### Frontend
- React 18
- Vite 5
- Native Web APIs (MediaRecorder, WebSocket, getUserMedia)

## ğŸ“– API Credits

- **Deepgram**: Speech-to-Text (Nova-2)
- **Google Gemini**: AI Brain (2.0 Flash-Lite)
- **Murf.ai**: Text-to-Speech (Falcon)
- **Pinecone**: Vector Database

## ğŸ’¡ Tips

1. **Low Latency**: 250ms timeslice is optimal balance
2. **Speech Detection**: speech_final prevents interrupting user
3. **Error Handling**: Always check WebSocket state before sending
4. **Audio Quality**: 128 kbps is good balance for voice
5. **Cleanup**: Always stop tracks when recording stops

## ğŸ¤ Contributing

Feel free to:
- Report bugs
- Suggest features
- Submit pull requests
- Improve documentation

## ğŸ“„ License

MIT License

---

**Built with â¤ï¸ for high-performance voice AI**

**Stack**: Node.js + React + Deepgram + Gemini + Murf + Pinecone
